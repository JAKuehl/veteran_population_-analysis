{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying which Veterans enroll in the VA\n",
    "---\n",
    "\n",
    "This is the biggest question.  If we can identify traits based on which veterans enroll and which do not could have impact on getting vets the help that they need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.svm import SVR, LinearSVR, SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../vets_5yr_clean.csv does not exist: '../vets_5yr_clean.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-31253f5909f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in dataset.  In this case the 5yr veteran dataset is being used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../vets_5yr_clean.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m'''Additional viewing options'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# This code sets the notebook to display maximum columns.  Uncomment it to see trunacted view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/dsi/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../vets_5yr_clean.csv does not exist: '../vets_5yr_clean.csv'"
     ]
    }
   ],
   "source": [
    "# Read in dataset.  In this case the 5yr veteran dataset is being used\n",
    "vet = pd.read_csv(\"../veteran_population_-analysis/working_data/vets_5yr_clean.csv\")\n",
    "\n",
    "'''Additional viewing options'''\n",
    "# This code sets the notebook to display maximum columns.  Uncomment it to see trunacted view\n",
    "pd.set_option('display.max_columns', len(vet))  \n",
    "\n",
    "# This resets the view to a truncated output\n",
    "#pd.reset_option('display.max_rows')\n",
    "\n",
    "vet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Groups\n",
    "---\n",
    "The PUMs dataset is incredibly feature rich.  However, this also makes makes it unwieldy when the desire is only test a specific group of features.  For the sake of simplication, I grouped all of the features into groups, to make for simpler modeling.  Additionally, there are 2 versions of the groups:\n",
    "\n",
    "* **Non-Encoded** - This uses all categorical features in their orginal state.\n",
    "* **One Hot Encoded** - This uses expanded categorical variables.\n",
    "\n",
    "#### Group Definitions\n",
    "\n",
    "* **Veteran Specific Features** = ```veteran``` \n",
    "* **Health and Well Being Features** = ```health```\n",
    "* **Employment and Income Features** = ```work_income```\n",
    "* **Ethnicity Features** = ```ethnicity```\n",
    "* **Lifestyle Features** = ```lifestyle```\n",
    "* **Geographic Location Features** = ```location```\n",
    "* **General Demographic Features** = ```general```\n",
    "* **Education Features** = ```education```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The feature names in this data set are genreally hard to read.  \n",
    "This code block puts them into lists of corresponding variables.'''\n",
    "\n",
    "# Non-encoded variables\n",
    "veteran = ['DRAT', 'DRATX', 'HINS6', 'MIL', 'MLPA', 'MLPB', 'MLPCD', 'MLPE', 'MLPFG', 'MLPH', 'MLPI', 'MLPJ', 'MLPK',\n",
    "          'VPS', 'FHINS5C', 'FHINS5P', 'FHINS6P', 'FMILSP']\n",
    "health = ['DDRS', 'DEAR', 'DEYE', 'DOUT', 'DPHY', 'DRAT', 'DREM', 'HINS1', 'HINS2', 'HINS3', 'HINS4', 'HINS5', 'HINS6', \n",
    "          'MIG', 'DIS', 'HICOV']\n",
    "work_income = ['COW', 'INTP_adj', 'OIP_adj', 'NWAB', 'NWLK', 'PAP_adj', 'RETP_adj', 'SEMP_adj', 'SSIP_adj', 'SSP_adj', \n",
    "              'WAGP_adj', 'WKHP', 'WKL', 'WKW', 'WRK', 'PINCP', 'PERNP', 'POVPIP', 'ESR']\n",
    "ethnicity = ['CIT', 'ENG', 'HISP', 'RAC1P', 'RACAIAN', 'RACASN', 'RACBLK', 'RACNH', 'RACNUM', 'RACPI', 'RACSOR', 'RACWHT', \n",
    "            'FCITP', 'LANX']\n",
    "lifestyle = ['JWRIP_1', 'JWRIP_2', 'MAR_1',\t'MAR_2', 'MAR_3', 'MAR_4', 'MARHD', 'MARHT', 'MARHW', 'MSP']\n",
    "location = ['DIVISION', 'REGION', 'ST', 'PUMA', 'intercept']\n",
    "general = ['AGEP', 'MAR', 'SEX']\n",
    "education = ['SCH', 'SCHL', 'SCIENGP', 'SCIENGRLP']\n",
    "\n",
    "# One Hot Encoded variables\n",
    "veteran_e = ['DRAT', 'DRATX_2', 'HINS6_2', 'MIL_2', 'MIL_3', 'MLPA_0', 'MLPA_1', 'MLPB_0', 'MLPB_1', 'MLPCD_0', 'MLPCD_1', 'MLPE_0',\n",
    "             'MLPE_1', 'MLPFG_0', 'MLPFG_1', 'MLPH_0', 'MLPH_1', 'MLPI_0', 'MLPI_1', 'MLPJ_0', 'MLPJ_1', 'MLPK_0', 'MLPK_1',\n",
    "             'VPS_1', 'VPS_2', 'VPS_3', 'VPS_4', 'VPS_5', 'VPS_6', 'VPS_7', 'VPS_8', 'VPS_9', 'VPS_10', 'VPS_11', 'VPS_12', \n",
    "             'VPS_13', 'VPS_14', 'VPS_15', 'ESR_1',\t'ESR_2', 'ESR_3', 'ESR_4', 'ESR_5', 'FHINS5C', 'FHINS5P', 'FHINS6P', \n",
    "             'FMILSP']\n",
    "health_e = ['DDRS', 'DEAR_2', 'DEYE', 'DOUT_2', 'DPHY_2', 'DRAT', 'DREM_2', 'HINS1_2', 'HINS2_2', 'HINS3_2', 'HINS4_2', \n",
    "            'HINS5_2', 'HINS6_2', 'DIS', 'HICOV_2']\n",
    "work_income_e = ['COW_1', 'COW_2', 'COW_3', 'COW_4', 'COW_5', 'COW_6', 'COW_7', 'COW_8', 'COW_9', 'INTP_adj', 'OIP_adj', \n",
    "                  'NWLK_1', 'NWLK_2', 'PAP_adj', 'RETP_adj', 'SEMP_adj', 'SSIP_adj', 'SSP_adj', \n",
    "                 'WAGP_adj', 'WKHP', 'WKL', 'WKW_1', 'WKW_2', 'WKW_3', 'WKW_4', 'WKW_5', 'WKW_6', 'WRK_1', 'WRK_2', \n",
    "                 'PINCP_adj', 'PERNP_adj', 'POVPIP', 'ESR_1', 'ESR_2', 'ESR_3', 'ESR_4', 'ESR_5']\n",
    "ethnicity_e = ['CIT_2', 'CIT_3', 'CIT_4', 'CIT_5', 'ENG_1', 'ENG_2', 'ENG_3', 'ENG_4', 'HISP', 'RAC1P', 'RACAIAN', 'RACASN', \n",
    "               'RACBLK', 'RACNH', 'RACNUM', 'RACSOR', 'RACWHT', 'FCITP', 'LANX']\n",
    "lifestyle_e = ['JWRIP', 'MAR_1', 'MAR_2', 'MAR_3', 'MAR_4', 'MARHD_1', 'MARHD_2', 'MARHT_1', 'MARHT_2', 'MARHT_3', \n",
    "               'MARHW_1', 'MARHW_2', 'MSP_1', 'MSP_2', 'MSP_3', 'MSP_4', 'MSP_5']\n",
    "location_e = ['DIVISION_2', 'DIVISION_3', 'DIVISION_4', 'DIVISION_5', 'DIVISION_6', 'DIVISION_7', 'DIVISION_8', \n",
    "              'DIVISION_9', 'REGION_2', 'REGION_3', 'REGION_4', 'ST', 'PUMA']\n",
    "general_e = ['AGEP', 'MAR_1', 'MAR_2', 'MAR_3', 'MAR_4', 'SEX_2']\n",
    "education_e = [#'SCH_2', 'SCH_3', 'SCH_2', 'SCH_3', 'SCHL_2', 'SCHL_3', 'SCHL_4', 'SCHL_5', 'SCHL_6', 'SCHL_7', \n",
    "               'SCHL_8', 'SCHL_9', 'SCHL_10', 'SCHL_11', 'SCHL_12', 'SCHL_13', 'SCHL_14', 'SCHL_15', 'SCHL_16', \n",
    "               'SCHL_17', 'SCHL_18', 'SCHL_19', 'SCHL_20', 'SCHL_21', 'SCHL_22', 'SCHL_23', 'SCHL_24', 'SCIENGP', \n",
    "               'SCIENGRLP']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vet['HINS6_2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for running various classifier models and scoring the results\n",
    "---\n",
    "There are a lot of vairables to test and this is will simplify the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that takes a set of dependent and independent features and splits and scales them\n",
    "\n",
    "def split_scaler(X, y):\n",
    "\n",
    "    #Train, test, split\n",
    "    indep_train, indep_test, dep_train, dep_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,  # both veterans and non VA enrollees are highly unbalanced\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=42)\n",
    "    # Scale data\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # Fit and transform, while keping column labels\n",
    "    indep_train_sc = pd.DataFrame(sc.fit_transform(indep_train), columns=indep_train.columns, index=indep_train.index)\n",
    "    indep_test_sc = pd.DataFrame(sc.transform(indep_test), columns=indep_test.columns, index=indep_test.index)\n",
    "    \n",
    "    # Instantiate PCA reduce the dimensionality of the features\n",
    "    pca = PCA(random_state = 69) \n",
    "    \n",
    "    # Fit PCA on the training data, while keping column labels\n",
    "    Z_train = pd.DataFrame(pca.fit(indep_train_sc), columns=indep_train.columns, index=indep_train.index)\n",
    "    Z_test = pd.DataFrame(pca.transform(indep_test), columns=indep_test.columns, index=indep_test.index)\n",
    "    \n",
    "    return dep_train, dep_test, indep_train_sc, indep_test_sc, Z_train, Z_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model, dep_train, dep_test, indep_train, indep_test):\n",
    "    \n",
    "    #Instantiate\n",
    "    model_i = model\n",
    "    \n",
    "    #Fit the model\n",
    "    model_fit = model_i.fit(indep_train, dep_train)\n",
    "    intercept = model_fit.intercept_\n",
    "    # Cross Validate\n",
    "    cv = cross_val_score(model_i, indep_train, dep_train, cv=7).mean()\n",
    "    \n",
    "    # Score the model\n",
    "    print(f\" Training Score :                      {model_fit.score(indep_train, dep_train)}\")\n",
    "    print(f\" Test Score :                          {model_fit.score(indep_test, dep_test)}\")\n",
    "    print(f\" Cross Valditation Score:              {cv}\")\n",
    "    print(f\" Intercept:                            {intercept}\")\n",
    "    \n",
    "    # Generate predictions.\n",
    "    preds = model_fit.predict(indep_test)\n",
    "    \n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "    plot_confusion_matrix(model_fit, \n",
    "                          indep_train, \n",
    "                          dep_train, \n",
    "                          cmap=\"Purples\", \n",
    "                          values_format='4g',\n",
    "                          ax=ax1);\n",
    "    ax1.set_title('Train Confusion Matrix', fontsize=14);\n",
    "\n",
    "    plot_confusion_matrix(model_fit, \n",
    "                          indep_test, \n",
    "                          dep_test, \n",
    "                          cmap=\"Oranges\", \n",
    "                          values_format='4g',\n",
    "                          ax=ax2);\n",
    "    ax2.set_title('Test Confusion Matrix', fontsize=14);\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Match coefficients to variables\n",
    "    var_coef = pd.DataFrame(list(zip(indep_train, model_fit.coef_)), columns=['Indy Variable', 'Coefficient'])\n",
    "    \n",
    "    #Classification report on test\n",
    "    print('')\n",
    "    print('')\n",
    "    print('CLASSIFICATION REPORT')\n",
    "    print(classification_report(dep_test, preds))\n",
    "    print('')\n",
    "    #print([pd.DataFrame(var_coef)])\n",
    "    \n",
    "    return var_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_prob(model):\n",
    "    \n",
    "    # Display Odds and Probablities of results\n",
    "    results = pd.DataFrame(model.params)\n",
    "        \n",
    "    results['Odds'] = np.exp(model.params)\n",
    "    results['Probability'] = results['Odds']/(1+results['Odds'])\n",
    "    \n",
    "    # Rename columns\n",
    "    results = results.reset_index()   \n",
    "    results.columns = {'index':'Name', 0:'Logit', 'Odds':'Odds', 'Probability':'Probability'}\n",
    "    print('Odds veteran will NOT enroll: \\n', '\\n', )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Data\n",
    "---\n",
    "During exploratory research, several test sets of features were modeled using a variety of classifier models. \n",
    "\n",
    "1. Logisitic Regression\n",
    "2. K Nearest Neighbors\n",
    "3. Random Forest\n",
    "4. Gaussian Naive Bayes\n",
    "5. Bernoulli Naive Bayes\n",
    "6. Multinomial Naive Bayes\n",
    "7. Decision Trees\n",
    "8. Random Forest\n",
    "9. Extra Trees\n",
    "10. ADA Boost\n",
    "11. Guassian Process\n",
    "12. Support Vector Classifier\n",
    "13. Linear Support Vector Classifier\n",
    "\n",
    "In the end, Logistic Regression consistently out performed the other models. It demonstrated to fit better, minimize variance, take less computing power and deliver interpretable results.  Therefore, the preferred classifier going forward is Logistic Regression.\n",
    "\n",
    "Additionally, as part of model improvement, any features that did not prove to be statistically significant were removed from successive models.  These removed variables are recorded in the ```.drop``` portion of the ```features``` variable for each model.\n",
    "\n",
    "## Classify VA enrollment based on Veteran Specific Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building modeling criteria\n",
    "features = vet[vet[veteran_e].drop(['HINS6_2', 'MLPA_0', 'MLPA_1', 'MLPB_1', 'MLPB_0','VPS_1', 'VPS_2', \n",
    "                                        'VPS_3', 'VPS_4', 'VPS_5', 'VPS_6', 'VPS_7', 'VPS_8', 'VPS_9', 'VPS_10', \n",
    "                                        'VPS_11', 'VPS_12', 'VPS_13', 'VPS_14', 'VPS_15', 'MLPE_0', 'MLPE_1',\n",
    "                                        'MLPFG_0', 'MLPFG_1', 'MLPH_0', 'MLPH_1', 'MLPI_0', 'MLPI_1', 'MLPJ_0', \n",
    "                                        'MLPK_0', 'ESR_5', 'MLPJ_1', 'MLPK_1', 'ESR_2', 'ESR_3'], axis=1).columns]\n",
    "\n",
    "# Split and Scale\n",
    "y_train, y_test, X_train_sc, X_test_sc, Z_train, Z_test = split_scaler(features, vet['HINS6_2'])\n",
    "\n",
    "# Logitistice Regression (scaled)\n",
    "var_coef_vet = classifier(LogisticRegression(n_jobs=-1), y_train, y_test, X_train_sc, X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant for intercept analysis\n",
    "exog = sm.add_constant(X_train_sc)\n",
    "\n",
    "# Checking coeffiecients with Stats Models\n",
    "model = sm.Logit(y_train, exog).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_prob(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify VA enrollment based on Health and Wellness Features\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building modeling criteria\n",
    "features = vet[vet[health_e].drop(['HINS6_2', 'DEYE'], axis=1).columns]\n",
    "\n",
    "# Split and Scale\n",
    "y_train1, y_test1, X_train_sc1, X_test_sc1, Z_train1, Z_test1 = split_scaler(features, vet['HINS6_2'])\n",
    "\n",
    "# Logitistice Regression (scaled)\n",
    "var_coef_health = classifier(LogisticRegression(n_jobs=-1), y_train1, y_test1, X_train_sc1, X_test_sc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant for intercept analysis\n",
    "exog1 = sm.add_constant(X_train_sc1)\n",
    "\n",
    "# Checking coeffiecients with Stats Models\n",
    "model1 = sm.Logit(y_train1, exog1, missing='drop').fit()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_prob(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify VA enrollment based on Work and Income Features\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building modeling criteria\n",
    "features = vet[vet[work_income_e].drop(['COW_1', 'COW_2', 'COW_3', 'COW_4', 'COW_5', 'COW_6', 'COW_7', \n",
    "                                            'COW_8', 'COW_9', 'WKW_1', 'WKW_2', 'WKW_3', 'WKW_4', 'WKW_5', \n",
    "                                            'WKW_6', 'WKL', 'PERNP_adj', 'PAP_adj', 'RETP_adj', 'SEMP_adj',\n",
    "                                            'SSIP_adj', 'SSP_adj', 'WAGP_adj'], axis=1).columns]\n",
    "\n",
    "# Split and Scale\n",
    "y_train2, y_test2, X_train_sc2, X_test_sc2, Z_train2, Z_test2 = split_scaler(features, vet['HINS6_2'])\n",
    "\n",
    "# Logitistice Regression (scaled)\n",
    "var_coef_work = classifier(LogisticRegression(n_jobs=-1), y_train2, y_test2, X_train_sc2, X_test_sc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant for intercept analysis\n",
    "exog2 = sm.add_constant(X_train_sc2)\n",
    "\n",
    "# Checking coeffiecients with Stats Models\n",
    "model2 = sm.Logit(y_train2, exog2, missing='drop').fit()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_prob(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify VA enrollment based on Ethnicity Features\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building modeling criteria\n",
    "features = vet[vet[ethnicity_e].drop(['CIT_2', 'CIT_3', 'ENG_1', 'ENG_2', 'ENG_3', 'ENG_4', 'HISP', 'RAC1P',\n",
    "                                         'RACASN', 'RACNH', 'RACNUM', 'RACSOR', 'RACWHT', 'LANX'], \n",
    "                                         axis=1).columns]\n",
    "\n",
    "# Split and Scale\n",
    "y_train3, y_test3, X_train_sc3, X_test_sc3, Z_train3, Z_test3 = split_scaler(features, vet['HINS6_2'])\n",
    "\n",
    "# Logitistice Regression (scaled)\n",
    "var_coef_work = classifier(LogisticRegression(n_jobs=-1), y_train3, y_test3, X_train_sc3, X_test_sc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant for intercept analysis\n",
    "exog3 = sm.add_constant(X_train_sc3)\n",
    "\n",
    "# Checking coeffiecients with Stats Models\n",
    "model3 = sm.Logit(y_train3, exog3, missing='drop').fit()\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_prob(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify VA enrollment based on Lifestyle Features\n",
    "---\n",
    "None of these features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building modeling criteria\n",
    "features = vet[vet[lifestyle_e].drop(['MAR_1', 'MAR_2', 'MAR_4', 'MAR_3', 'MARHD_1', 'MARHD_2', 'MARHT_1',\n",
    "                                     'MARHT_2', 'MARHT_3', 'MARHW_1', 'MARHW_2', 'MSP_1', 'MSP_2', 'MSP_3',\n",
    "                                     'MSP_4', 'MSP_5'], axis=1).columns]\n",
    "\n",
    "# Split and Scale\n",
    "y_train4, y_test4, X_train_sc4, X_test_sc4, Z_train4, Z_test4 = split_scaler(features, vet['HINS6_2'])\n",
    "\n",
    "# Logitistice Regression (scaled)\n",
    "var_coef_work = classifier(LogisticRegression(n_jobs=-1), y_train4, y_test4, X_train_sc4, X_test_sc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant for intercept analysis\n",
    "exog4 = sm.add_constant(X_train_sc4)\n",
    "\n",
    "# Checking coeffiecients with Stats Models\n",
    "model4 = sm.Logit(y_train4, exog4, missing='drop').fit()\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_prob(model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify VA enrollment based on Location Features\n",
    "---\n",
    "Nothing of interest in this group.  Both state(ST) and PUMA are not true categorizations.  We can effectively say that location is not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building modeling criteria\n",
    "features = vet[vet[location_e].drop([], axis=1).columns]\n",
    "\n",
    "# Split and Scale\n",
    "y_train5, y_test5, X_train_sc5, X_test_sc5, Z_train5, Z_test5 = split_scaler(features, vet['HINS6_2'])\n",
    "\n",
    "# Logitistice Regression (scaled)\n",
    "var_coef_work = classifier(LogisticRegression(n_jobs=-1), y_train5, y_test5, X_train_sc5, X_test_sc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant for intercept analysis\n",
    "exog5 = sm.add_constant(X_train_sc5)\n",
    "\n",
    "# Checking coeffiecients with Stats Models\n",
    "model5 = sm.Logit(y_train5, exog5, missing='drop').fit()\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_prob(model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify VA enrollment based on General Demographic Features\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building modeling criteria\n",
    "features = vet[vet[general_e].drop(['MAR_2'], axis=1).columns]\n",
    "\n",
    "# Split and Scale\n",
    "y_train6, y_test6, X_train_sc6, X_test_sc6, Z_train6, Z_test6 = split_scaler(features, vet['HINS6_2'])\n",
    "\n",
    "# Logitistice Regression (scaled)\n",
    "var_coef_work = classifier(LogisticRegression(n_jobs=-1), y_train6, y_test6, X_train_sc6, X_test_sc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant for intercept analysis\n",
    "exog6 = sm.add_constant(X_train_sc6)\n",
    "\n",
    "# Checking coeffiecients with Stats Models\n",
    "model6 = sm.Logit(y_train6, exog6, missing='drop').fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_prob(model6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify VA enrollment based on Education Features\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building modeling criteria\n",
    "features = vet[vet[education_e].drop(['SCHL_8', 'SCHL_9', 'SCHL_10', 'SCHL_14', 'SCHL_16'], axis=1).columns]\n",
    "\n",
    "# Split and Scale\n",
    "y_train7, y_test7, X_train_sc7, X_test_sc7, Z_train7, Z_test7 = split_scaler(features, vet['HINS6_2'])\n",
    "\n",
    "# Logitistice Regression (scaled)\n",
    "var_coef_work = classifier(LogisticRegression(n_jobs=-1), y_train7, y_test7, X_train_sc7, X_test_sc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant for intercept analysis\n",
    "exog7 = sm.add_constant(X_train_sc7)\n",
    "\n",
    "# Checking coeffiecients with Stats Models\n",
    "model7 = sm.Logit(y_train7, exog7, missing='drop').fit()\n",
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_prob(model7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Combined Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building modeling criteria for the top performing features\n",
    "\n",
    "top_features = ['DRAT', 'DRATX', 'MIL', 'MLPCD_0', 'MLPCD_1', 'ESR_1', 'ESR_4',   #VETERAN FEATURES\n",
    "                'FHINS5C', 'FHINS5P', 'FMILSP', \n",
    "                'DDRS', 'DEAR_2', 'DEYE', 'DOUT_2', 'DPHY_2', 'DREM_2', 'HINS1_2',#HEALTH FEATURES\n",
    "                'HINS2_2', 'HINS3_2', 'HINS4_2', 'HINS5_2', 'DIS', 'HICOV_2', \n",
    "                'INTP_adj', 'OIP_adj', 'PINCP_adj', 'NWLK_1', 'NWLK_2', 'WKHP',   #WORK/INCOME FEATURES\n",
    "                'WRK_1', 'WRK_2', 'POVPIP', 'ESR_2', 'ESR_3', 'ESR_5',\n",
    "                'CIT_4', 'CIT_5', 'RACBLK', 'FCITP', 'RACAIAN',                   #ETHNICITY FEATURES\n",
    "                'AGEP', 'MAR_1', 'MAR_4', 'SEX_2',                                #GENERAL FEATURES\n",
    "                'SCHL_11', 'SCHL_12', 'SCHL_13', 'SCHL_17', 'SCHL_18', 'SCHL_19', #EDUCATION FEATURES\n",
    "                'SCHL_20', 'SCHL_21', 'SCHL_22', 'SCHL_23', 'SCHL_24', 'SCIENGP']\n",
    "\n",
    "features = vet[vet[top_features].drop(['ESR_1', 'ESR_4','FHINS5C', 'DEYE', 'HINS5_2', 'WKHP', 'CIT_5',\n",
    "                                      'MAR_4', 'SCHL_11', 'SCHL_12', 'SCHL_13', 'INTP_adj'], axis=1).columns]\n",
    "\n",
    "# Split and Scale\n",
    "y_train8, y_test8, X_train_sc8, X_test_sc8, Z_train8, Z_test8 = split_scaler(features, vet['HINS6_2'])\n",
    "\n",
    "# Logitistice Regression (scaled)\n",
    "var_coef_work = classifier(LogisticRegression(n_jobs=-1), y_train8, y_test8, X_train_sc8, X_test_sc8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a constant for intercept analysis\n",
    "exog8 = sm.add_constant(X_train_sc8)\n",
    "\n",
    "# Checking coeffiecients with Stats Models\n",
    "model8 = sm.Logit(y_train8, exog8, missing='drop').fit()\n",
    "model8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = log_prob(model8)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#probs = results.drop(labels=['Odds']) #, axis=1)\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(results['index', 'Probability'], annot=True, cmap=\"rainbow\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at top preforming features that indicate a Canon post\n",
    "results = results.T\n",
    "probs = results.drop(labels=0, axis=1)\n",
    "top_features = results[results['index']]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4,8))\n",
    "sns.heatmap(top_features.sort_values(probs['Probability'], ascending=False), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
